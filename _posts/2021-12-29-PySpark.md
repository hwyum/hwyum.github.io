---
title: "PySpark 자주 사용하는 문법 정리"
date: 2021-12-29
tags: python
categories: python spark PySpark
---

주의: 아래 내용은 `PySpark 2.4.5` version 기준으로 작성되었습니다. 

# 1. groupBy aggregation 연산

특정 컬럼으로 groupBy 후, 다른 컬럼에 대한 distinct한 count값을 얻고 싶은 경우

```python
df = y.groupBy("year").agg(countDistinct("id"))
df.show()
```



groupBy 후 특정 컬럼에 대한 sum 값을 얻고 싶은 경우

```python
df = y.groupBy("year").agg(sum("count"))
df.show()
```



특정 조건에 해당하는 row에 대해서만 count를 하고 싶은 경우

```python
# column값이 0보다 큰 경우에만 count를 하는 경우
df.agg(F.count(F.when(F.col('col') > 0, True)))
df.show()
```

# 2. selectExpr

select문을 보다 풍부한 표현식과 함께 사용할 수 있는 방법

## 

# 3. Column명 변경

`withColumnRenamed()` 함수를 통해 컬럼명 변경 가능



# 4. approxQuantile

percentile값을 알고 싶을 때 사용할 수 있는 함수입니다. 

```python
# percentile 0.8, 0.9, 0.99 값을 알고 싶은 경우
df.approxQuantile('duration', [0.8, 0.9, 0.95, 0.99], relativeError=0.001)
```



# 5. json string으로 들어가있는 column에서 값 꺼내기

json string 에서 값을 꺼내려면 다음과 같이 get_json_object 를 사용한다. (https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.get_json_object.html)
**result = jsonDF.select(
  get_json_object(col("jsonString"), "$.myJSONKey.myJSONValue[1]").alias("result")
)**
