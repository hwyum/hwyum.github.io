---
title: "Tabular Data: Deep Learning is Not All You Need (2022)"
date: 2022-06-01
tags:
    - ML
    - Tabular Data
    - XGBoost
categories: 
    - ML
    - ML Papers
toc: true
toc_sticky: true
toc_label: "페이지 목차"
---

# Intrudoction



# Background

## Tabular Data에서의 딥러닝

2개의 메인 카테고리가 존재함. 

- Differentiable trees: 
  - DT를 미문 가능하게 만드는 방법을 찾는 것.
  - 기존 DT는 미분할 수 없어 gradient optimization이 불가능. => 내부 노드에서의 decision function을 스무딩해서 tree function과 tree rountingAttention-based models
- Attention-based models:
  - tabular deep network에 attention과 같은 모듈을 도입함. 
  - 최근의 연구들은, 'inter-sample' attention (주어진 샘플 내 피쳐간 interaction)과 'intra-sample attention'(데이터 포인트간의 interaction)을 모두 제안하였음. 
  - 예시:
    - TabNet (Arik and Pfister, 2021)
    - Tabtransformer (Huang, 2020)
- Etc. 
  - Regularization method: 대규모 하이퍼파라미터 튜닝 방식을 사용하여 모든 신경 가중치(neural weight)에 대해 "regularization strength"를 학습 (??!!)
  - Multiplicative interaction에 대한 명시적인 모델링: 여러 가지 방식으로 feature product를 MLP 모델로 통합하고자 함. 
  - 1D-CNN: covolution의 이점을 tabular data에서 활용함. 

그러나, 앞서 언급했듯이 아직 벤치마크 데이터셋이 없어서 모델간 비교가 잘 되고 있지는 않습니다. 



## 비교 대상 모델

본 연구에서는 4개의 모델을 고려했습니다. 

- TabNet (Arik and Pfister, 2021)
- NODE(Popov et al., 2020)
- DNF-Net (Katzir et al., 2021)
- 1D-CNN (Baosenguo, 2021)



### TabNet

